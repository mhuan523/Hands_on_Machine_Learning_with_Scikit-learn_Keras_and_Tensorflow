{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "tf.Tensor(0, shape=(), dtype=int32) 0\n",
      "tf.Tensor(1, shape=(), dtype=int32) 1\n",
      "tf.Tensor(2, shape=(), dtype=int32) 2\n",
      "tf.Tensor(3, shape=(), dtype=int32) 3\n",
      "tf.Tensor(4, shape=(), dtype=int32) 4\n",
      "tf.Tensor(5, shape=(), dtype=int32) 5\n",
      "tf.Tensor(6, shape=(), dtype=int32) 6\n",
      "tf.Tensor(7, shape=(), dtype=int32) 7\n",
      "tf.Tensor(8, shape=(), dtype=int32) 8\n",
      "tf.Tensor(9, shape=(), dtype=int32) 9\n"
     ]
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "print(dataset)\n",
    "for item in dataset:\n",
    "    print(item, item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (7,), types: tf.int32>\n",
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32) [0 1 2 3 4 5 6]\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32) [7 8 9 0 1 2 3]\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32) [4 5 6 7 8 9 0]\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32) [1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "print(dataset)\n",
    "for item in dataset:\n",
    "    print(item, item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ParallelMapDataset shapes: (7,), types: tf.int32>\n",
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32) [ 0  2  4  6  8 10 12]\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32) [14 16 18  0  2  4  6]\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32) [ 8 10 12 14 16 18  0]\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32) [ 2  4  6  8 10 12 14]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2, num_parallel_calls=4)\n",
    "print(dataset)\n",
    "for item in dataset:\n",
    "    print(item, item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_UnbatchDataset shapes: (), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(tf.data.Dataset.unbatch)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FilterDataset shapes: (), types: tf.int32>\n",
      "tf.Tensor(12, shape=(), dtype=int32) 12\n",
      "tf.Tensor(14, shape=(), dtype=int32) 14\n",
      "tf.Tensor(16, shape=(), dtype=int32) 16\n",
      "tf.Tensor(18, shape=(), dtype=int32) 18\n",
      "tf.Tensor(12, shape=(), dtype=int32) 12\n",
      "tf.Tensor(14, shape=(), dtype=int32) 14\n",
      "tf.Tensor(16, shape=(), dtype=int32) 16\n",
      "tf.Tensor(18, shape=(), dtype=int32) 18\n",
      "tf.Tensor(12, shape=(), dtype=int32) 12\n",
      "tf.Tensor(14, shape=(), dtype=int32) 14\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x > 10)\n",
    "print(dataset)\n",
    "for item in dataset:\n",
    "    print(item, item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12, shape=(), dtype=int32) 12\n",
      "tf.Tensor(14, shape=(), dtype=int32) 14\n",
      "tf.Tensor(16, shape=(), dtype=int32) 16\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item, item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (None,), types: tf.int64>\n",
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64) [0 2 3 6 7 9 4]\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64) [5 0 1 1 8 6 5]\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64) [4 8 7 1 2 3 0]\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64) [5 4 2 7 8 9 9]\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64) [3 6]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "print(dataset)\n",
    "for item in dataset:\n",
    "    print(item, item.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the California dataset to multiple CSV files\n",
    "Let's start by loading and preparing the California housing dataset. We first load it, then split it into a training set, a validation set and a test set, and finally we scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d099b1f74632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_all' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_all, X_test, y_train_all, y_test = \\\n",
    "    train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "print('X_mean:', X_mean)\n",
    "print('X_std:', X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. To demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths\n",
    "\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(test_filepaths[1]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
